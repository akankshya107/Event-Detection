{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/bhavana/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open('final_output.json') as f:\n",
    "    data = json.load(f)\n",
    "ls=[]\n",
    "for i in data['json']:\n",
    "    ls.append(i['text'])\n",
    "processed_docs = pd.Series( (ls) )\n",
    "#print type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d93e9e6f29f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatize example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemmer Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [fin, two, year, intens, plan, program, fin, b...\n",
       "1         [lov, twit, infotwitw, https, tcoo5atsnfd2r]\n",
       "2    [hello, cal, gymbot, want, inspir, look, incre...\n",
       "3            [mf, mov, say, cus, https, tcopw0q2plrdg]\n",
       "4    [kind, retweet, anyon, victim, tim, also, on, ...\n",
       "5    [noishqbaaazwithoutsurbh, acknowledg, credit, ...\n",
       "6                               [mat, com, expery, ag]\n",
       "7    [sur, michael, avenatt, attorney, adv, cli, ma...\n",
       "8    [send, bts, mem, airdrop, cinem, mak, new, arm...\n",
       "9    [angl, vmin, koal, hug, today, ev, bet, clear,...\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65114, u'vickycuba1')\n",
      "(70915, u'one thousand, nine hundred and forty-on')\n",
      "(55930, u'viralgreek')\n",
      "(65498, u'tco22rkjxz2ec')\n",
      "(48346, u'tcoambiop5ntl')\n",
      "(40288, u'two hundred and eighty-two thousand and eighteen')\n",
      "(1308, u'seven hundred and forty-one thousand, seven hundred and forty-one')\n",
      "(13897, u'mustachio')\n",
      "(36463, u'tcozj4wopdtjs')\n",
      "(11688, u'spiderm')\n",
      "(68488, u'cityvil')\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59646\n"
     ]
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "print len(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 119 (\"lat\") appears 1 time.\n",
      "Word 561 (\"yeah\") appears 1 time.\n",
      "Word 578 (\"not\") appears 1 time.\n",
      "Word 1852 (\"main\") appears 1 time.\n",
      "Word 1982 (\"w\") appears 1 time.\n",
      "Word 2181 (\"mut\") appears 1 time.\n",
      "Word 4614 (\"interact\") appears 1 time.\n",
      "Word 49258 (\"nellucnhod\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_4310 = bow_corpus[59645]\n",
    "\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "                                                     dictionary[bow_doc_4310[i][0]], \n",
    "                                                     bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.23466613948424303), (1, 0.32594753925114506), (2, 0.39740129967727), (3, 0.017096452934945015), (4, 0.3301679315502826), (5, 0.3728288061042494), (6, 0.24625244523534862), (7, 0.29554152473320355), (8, 0.44872100872292164), (9, 0.1702301521914007), (10, 0.15321040496351998), (11, 0.1751296807017823)]\n"
     ]
    }
   ],
   "source": [
    "print corpus_tfidf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.23466613948424303),\n",
      " (1, 0.32594753925114506),\n",
      " (2, 0.39740129967727),\n",
      " (3, 0.017096452934945015),\n",
      " (4, 0.3301679315502826),\n",
      " (5, 0.3728288061042494),\n",
      " (6, 0.24625244523534862),\n",
      " (7, 0.29554152473320355),\n",
      " (8, 0.44872100872292164),\n",
      " (9, 0.1702301521914007),\n",
      " (10, 0.15321040496351998),\n",
      " (11, 0.1751296807017823)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running LDA using Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.6055469513\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t0=time.time()\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=15, id2word=dictionary, passes=2, workers=2)\n",
    "t1=time.time()\n",
    "total=t1-t0\n",
    "print total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.070*\"https\" + 0.014*\"day\" + 0.013*\"happy\" + 0.009*\"lov\" + 0.008*\"look\" + 0.008*\"friend\" + 0.006*\"new\" + 0.006*\"good\" + 0.006*\"cut\" + 0.005*\"go\"\n",
      "Topic: 1 \n",
      "Words: 0.072*\"https\" + 0.010*\"go\" + 0.009*\"ask\" + 0.008*\"year\" + 0.007*\"diff\" + 0.006*\"wait\" + 0.005*\"girl\" + 0.005*\"liv\" + 0.005*\"zero\" + 0.005*\"guy\"\n",
      "Topic: 2 \n",
      "Words: 0.065*\"https\" + 0.008*\"got\" + 0.007*\"lif\" + 0.007*\"mak\" + 0.007*\"us\" + 0.006*\"tim\" + 0.006*\"enough\" + 0.005*\"wom\" + 0.005*\"men\" + 0.005*\"snow\"\n",
      "Topic: 3 \n",
      "Words: 0.070*\"https\" + 0.015*\"video\" + 0.011*\"youtub\" + 0.010*\"ad\" + 0.009*\"new\" + 0.007*\"watch\" + 0.007*\"get\" + 0.006*\"lik\" + 0.006*\"play\" + 0.006*\"cal\"\n",
      "Topic: 4 \n",
      "Words: 0.069*\"https\" + 0.015*\"get\" + 0.010*\"nee\" + 0.009*\"us\" + 0.007*\"think\" + 0.006*\"last\" + 0.006*\"start\" + 0.005*\"first\" + 0.005*\"driv\" + 0.005*\"tim\"\n",
      "Topic: 5 \n",
      "Words: 0.060*\"https\" + 0.009*\"on\" + 0.007*\"get\" + 0.007*\"sad\" + 0.006*\"two\" + 0.006*\"new\" + 0.006*\"us\" + 0.006*\"two thousand and nineteen\" + 0.006*\"season\" + 0.006*\"hour\"\n",
      "Topic: 6 \n",
      "Words: 0.033*\"https\" + 0.018*\"u\" + 0.017*\"good\" + 0.014*\"na\" + 0.012*\"lik\" + 0.012*\"fuck\" + 0.009*\"shit\" + 0.009*\"ev\" + 0.008*\"on\" + 0.008*\"gon\"\n",
      "Topic: 7 \n",
      "Words: 0.079*\"https\" + 0.012*\"keep\" + 0.008*\"someon\" + 0.007*\"ev\" + 0.007*\"peopl\" + 0.006*\"lik\" + 0.006*\"day\" + 0.006*\"know\" + 0.006*\"scroll\" + 0.006*\"problem\"\n",
      "Topic: 8 \n",
      "Words: 0.067*\"https\" + 0.017*\"on\" + 0.010*\"exo\" + 0.007*\"thank\" + 0.007*\"wel\" + 0.007*\"taught\" + 0.006*\"win\" + 0.006*\"mus\" + 0.006*\"lov\" + 0.005*\"tempo2ndwin\"\n",
      "Topic: 9 \n",
      "Words: 0.072*\"https\" + 0.010*\"lik\" + 0.009*\"win\" + 0.007*\"try\" + 0.005*\"com\" + 0.005*\"feel\" + 0.005*\"chant\" + 0.005*\"get\" + 0.004*\"day\" + 0.004*\"week\"\n",
      "Topic: 10 \n",
      "Words: 0.076*\"https\" + 0.011*\"god\" + 0.010*\"on\" + 0.010*\"check\" + 0.010*\"find\" + 0.009*\"person\" + 0.006*\"someon\" + 0.006*\"perfect\" + 0.006*\"thank\" + 0.006*\"shar\"\n",
      "Topic: 11 \n",
      "Words: 0.054*\"https\" + 0.033*\"follow\" + 0.021*\"lik\" + 0.019*\"retweet\" + 0.012*\"rt\" + 0.010*\"tweet\" + 0.010*\"everyon\" + 0.010*\"on\" + 0.009*\"two\" + 0.006*\"think\"\n",
      "Topic: 12 \n",
      "Words: 0.062*\"https\" + 0.020*\"lov\" + 0.016*\"nt\" + 0.014*\"nev\" + 0.009*\"tak\" + 0.008*\"ca\" + 0.008*\"back\" + 0.007*\"on\" + 0.007*\"see\" + 0.007*\"want\"\n",
      "Topic: 13 \n",
      "Words: 0.074*\"https\" + 0.008*\"ev\" + 0.007*\"mom\" + 0.006*\"visit\" + 0.006*\"mad\" + 0.006*\"off\" + 0.006*\"peopl\" + 0.005*\"word\" + 0.005*\"liv\" + 0.005*\"victim\"\n",
      "Topic: 14 \n",
      "Words: 0.082*\"say\" + 0.063*\"https\" + 0.061*\"nt\" + 0.012*\"brain\" + 0.008*\"on\" + 0.007*\"vot\" + 0.005*\"dont\" + 0.005*\"best\" + 0.004*\"real\" + 0.004*\"lik\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Can you distinguish different topics using the words in each topic and their corresponding weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running LDA using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.2882111073\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t0=time.time()\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n",
    "t1=time.time()\n",
    "total=t1-t0\n",
    "print total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.003*\"https\" + 0.003*\"lov\" + 0.002*\"com\" + 0.002*\"cut\" + 0.002*\"lik\" + 0.002*\"nt\" + 0.002*\"peopl\" + 0.002*\"see\" + 0.002*\"us\" + 0.001*\"look\"\n",
      "Topic: 1 Word: 0.004*\"say\" + 0.003*\"nt\" + 0.003*\"https\" + 0.002*\"im\" + 0.002*\"right\" + 0.002*\"ev\" + 0.002*\"know\" + 0.002*\"shit\" + 0.002*\"hurt\" + 0.002*\"tim\"\n",
      "Topic: 2 Word: 0.003*\"https\" + 0.003*\"lif\" + 0.003*\"say\" + 0.002*\"thank\" + 0.002*\"on\" + 0.002*\"get\" + 0.002*\"lik\" + 0.002*\"exo\" + 0.002*\"happy\" + 0.002*\"watch\"\n",
      "Topic: 3 Word: 0.004*\"lov\" + 0.003*\"https\" + 0.003*\"nt\" + 0.002*\"fuck\" + 0.002*\"cant\" + 0.002*\"twit\" + 0.002*\"u\" + 0.002*\"god\" + 0.002*\"cal\" + 0.002*\"ask\"\n",
      "Topic: 4 Word: 0.008*\"say\" + 0.005*\"nt\" + 0.003*\"https\" + 0.003*\"on\" + 0.002*\"brain\" + 0.002*\"best\" + 0.002*\"real\" + 0.002*\"tak\" + 0.002*\"friday\" + 0.002*\"two thousand and eighteen\"\n",
      "Topic: 5 Word: 0.003*\"https\" + 0.003*\"got\" + 0.003*\"on\" + 0.003*\"lov\" + 0.003*\"friend\" + 0.003*\"us\" + 0.002*\"peopl\" + 0.002*\"get\" + 0.002*\"last\" + 0.002*\"nee\"\n",
      "Topic: 6 Word: 0.004*\"day\" + 0.003*\"lik\" + 0.003*\"https\" + 0.003*\"nev\" + 0.003*\"lov\" + 0.002*\"get\" + 0.002*\"want\" + 0.002*\"on\" + 0.002*\"na\" + 0.002*\"someon\"\n",
      "Topic: 7 Word: 0.008*\"follow\" + 0.006*\"retweet\" + 0.005*\"everyon\" + 0.005*\"lik\" + 0.003*\"thing\" + 0.003*\"https\" + 0.003*\"army\" + 0.002*\"liv\" + 0.002*\"sometim\" + 0.002*\"ev\"\n",
      "Topic: 8 Word: 0.003*\"https\" + 0.003*\"on\" + 0.002*\"check\" + 0.002*\"person\" + 0.002*\"autom\" + 0.002*\"u\" + 0.002*\"new\" + 0.002*\"bless\" + 0.002*\"season\" + 0.002*\"thank\"\n",
      "Topic: 9 Word: 0.005*\"scroll\" + 0.005*\"scrolling\" + 0.005*\"keep\" + 0.005*\"tcoksmdoryhet\" + 0.003*\"https\" + 0.003*\"get\" + 0.002*\"nt\" + 0.002*\"ev\" + 0.002*\"someon\" + 0.002*\"pleas\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification of the topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation by classifying sample document using LDA Bag of Words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'arun',\n",
       " u'jaitley',\n",
       " u'pow',\n",
       " u'speech',\n",
       " u'med',\n",
       " u'nat',\n",
       " u'award',\n",
       " u'excel',\n",
       " u'journ',\n",
       " u'l',\n",
       " u'cvr',\n",
       " u'new',\n",
       " u'https',\n",
       " u'tcob9fm0dzatp',\n",
       " u'via']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.688888847828\t \n",
      "Topic: 0.082*\"say\" + 0.063*\"https\" + 0.061*\"nt\" + 0.012*\"brain\" + 0.008*\"on\" + 0.007*\"vot\" + 0.005*\"dont\" + 0.005*\"best\" + 0.004*\"real\" + 0.004*\"lik\"\n",
      "\n",
      "Score: 0.0222222525626\t \n",
      "Topic: 0.076*\"https\" + 0.011*\"god\" + 0.010*\"on\" + 0.010*\"check\" + 0.010*\"find\" + 0.009*\"person\" + 0.006*\"someon\" + 0.006*\"perfect\" + 0.006*\"thank\" + 0.006*\"shar\"\n",
      "\n",
      "Score: 0.0222222264856\t \n",
      "Topic: 0.072*\"https\" + 0.010*\"go\" + 0.009*\"ask\" + 0.008*\"year\" + 0.007*\"diff\" + 0.006*\"wait\" + 0.005*\"girl\" + 0.005*\"liv\" + 0.005*\"zero\" + 0.005*\"guy\"\n",
      "\n",
      "Score: 0.0222222227603\t \n",
      "Topic: 0.070*\"https\" + 0.014*\"day\" + 0.013*\"happy\" + 0.009*\"lov\" + 0.008*\"look\" + 0.008*\"friend\" + 0.006*\"new\" + 0.006*\"good\" + 0.006*\"cut\" + 0.005*\"go\"\n",
      "\n",
      "Score: 0.0222222227603\t \n",
      "Topic: 0.065*\"https\" + 0.008*\"got\" + 0.007*\"lif\" + 0.007*\"mak\" + 0.007*\"us\" + 0.006*\"tim\" + 0.006*\"enough\" + 0.005*\"wom\" + 0.005*\"men\" + 0.005*\"snow\"\n",
      "\n",
      "Score: 0.0222222227603\t \n",
      "Topic: 0.070*\"https\" + 0.015*\"video\" + 0.011*\"youtub\" + 0.010*\"ad\" + 0.009*\"new\" + 0.007*\"watch\" + 0.007*\"get\" + 0.006*\"lik\" + 0.006*\"play\" + 0.006*\"cal\"\n",
      "\n",
      "Score: 0.0222222227603\t \n",
      "Topic: 0.069*\"https\" + 0.015*\"get\" + 0.010*\"nee\" + 0.009*\"us\" + 0.007*\"think\" + 0.006*\"last\" + 0.006*\"start\" + 0.005*\"first\" + 0.005*\"driv\" + 0.005*\"tim\"\n",
      "\n",
      "Score: 0.0222222227603\t \n",
      "Topic: 0.060*\"https\" + 0.009*\"on\" + 0.007*\"get\" + 0.007*\"sad\" + 0.006*\"two\" + 0.006*\"new\" + 0.006*\"us\" + 0.006*\"two thousand and nineteen\" + 0.006*\"season\" + 0.006*\"hour\"\n",
      "\n",
      "Score: 0.0222222227603\t \n",
      "Topic: 0.033*\"https\" + 0.018*\"u\" + 0.017*\"good\" + 0.014*\"na\" + 0.012*\"lik\" + 0.012*\"fuck\" + 0.009*\"shit\" + 0.009*\"ev\" + 0.008*\"on\" + 0.008*\"gon\"\n",
      "\n",
      "Score: 0.0222222227603\t \n",
      "Topic: 0.079*\"https\" + 0.012*\"keep\" + 0.008*\"someon\" + 0.007*\"ev\" + 0.007*\"peopl\" + 0.006*\"lik\" + 0.006*\"day\" + 0.006*\"know\" + 0.006*\"scroll\" + 0.006*\"problem\"\n",
      "\n",
      "Score: 0.0222222227603\t \n",
      "Topic: 0.067*\"https\" + 0.017*\"on\" + 0.010*\"exo\" + 0.007*\"thank\" + 0.007*\"wel\" + 0.007*\"taught\" + 0.006*\"win\" + 0.006*\"mus\" + 0.006*\"lov\" + 0.005*\"tempo2ndwin\"\n",
      "\n",
      "Score: 0.0222222227603\t \n",
      "Topic: 0.072*\"https\" + 0.010*\"lik\" + 0.009*\"win\" + 0.007*\"try\" + 0.005*\"com\" + 0.005*\"feel\" + 0.005*\"chant\" + 0.005*\"get\" + 0.004*\"day\" + 0.004*\"week\"\n",
      "\n",
      "Score: 0.0222222227603\t \n",
      "Topic: 0.054*\"https\" + 0.033*\"follow\" + 0.021*\"lik\" + 0.019*\"retweet\" + 0.012*\"rt\" + 0.010*\"tweet\" + 0.010*\"everyon\" + 0.010*\"on\" + 0.009*\"two\" + 0.006*\"think\"\n",
      "\n",
      "Score: 0.0222222227603\t \n",
      "Topic: 0.062*\"https\" + 0.020*\"lov\" + 0.016*\"nt\" + 0.014*\"nev\" + 0.009*\"tak\" + 0.008*\"ca\" + 0.008*\"back\" + 0.007*\"on\" + 0.007*\"see\" + 0.007*\"want\"\n",
      "\n",
      "Score: 0.0222222227603\t \n",
      "Topic: 0.074*\"https\" + 0.008*\"ev\" + 0.007*\"mom\" + 0.006*\"visit\" + 0.006*\"mad\" + 0.006*\"off\" + 0.006*\"peopl\" + 0.005*\"word\" + 0.005*\"liv\" + 0.005*\"victim\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[814]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test document has the highest probability to be part of the topic on the top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation by classifying sample document using LDA TF-IDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.406315296888\t \n",
      "Topic: 0.008*\"follow\" + 0.006*\"retweet\" + 0.005*\"everyon\" + 0.005*\"lik\" + 0.003*\"thing\" + 0.003*\"https\" + 0.003*\"army\" + 0.002*\"liv\" + 0.002*\"sometim\" + 0.002*\"ev\"\n",
      "\n",
      "Score: 0.326973795891\t \n",
      "Topic: 0.008*\"say\" + 0.005*\"nt\" + 0.003*\"https\" + 0.003*\"on\" + 0.002*\"brain\" + 0.002*\"best\" + 0.002*\"real\" + 0.002*\"tak\" + 0.002*\"friday\" + 0.002*\"two thousand and eighteen\"\n",
      "\n",
      "Score: 0.0333484299481\t \n",
      "Topic: 0.004*\"lov\" + 0.003*\"https\" + 0.003*\"nt\" + 0.002*\"fuck\" + 0.002*\"cant\" + 0.002*\"twit\" + 0.002*\"u\" + 0.002*\"god\" + 0.002*\"cal\" + 0.002*\"ask\"\n",
      "\n",
      "Score: 0.033346708864\t \n",
      "Topic: 0.003*\"https\" + 0.003*\"lov\" + 0.002*\"com\" + 0.002*\"cut\" + 0.002*\"lik\" + 0.002*\"nt\" + 0.002*\"peopl\" + 0.002*\"see\" + 0.002*\"us\" + 0.001*\"look\"\n",
      "\n",
      "Score: 0.0333391167223\t \n",
      "Topic: 0.004*\"day\" + 0.003*\"lik\" + 0.003*\"https\" + 0.003*\"nev\" + 0.003*\"lov\" + 0.002*\"get\" + 0.002*\"want\" + 0.002*\"on\" + 0.002*\"na\" + 0.002*\"someon\"\n",
      "\n",
      "Score: 0.0333388857543\t \n",
      "Topic: 0.005*\"scroll\" + 0.005*\"scrolling\" + 0.005*\"keep\" + 0.005*\"tcoksmdoryhet\" + 0.003*\"https\" + 0.003*\"get\" + 0.002*\"nt\" + 0.002*\"ev\" + 0.002*\"someon\" + 0.002*\"pleas\"\n",
      "\n",
      "Score: 0.0333356969059\t \n",
      "Topic: 0.004*\"say\" + 0.003*\"nt\" + 0.003*\"https\" + 0.002*\"im\" + 0.002*\"right\" + 0.002*\"ev\" + 0.002*\"know\" + 0.002*\"shit\" + 0.002*\"hurt\" + 0.002*\"tim\"\n",
      "\n",
      "Score: 0.0333348996937\t \n",
      "Topic: 0.003*\"https\" + 0.003*\"got\" + 0.003*\"on\" + 0.003*\"lov\" + 0.003*\"friend\" + 0.003*\"us\" + 0.002*\"peopl\" + 0.002*\"get\" + 0.002*\"last\" + 0.002*\"nee\"\n",
      "\n",
      "Score: 0.033333864063\t \n",
      "Topic: 0.003*\"https\" + 0.003*\"lif\" + 0.003*\"say\" + 0.002*\"thank\" + 0.002*\"on\" + 0.002*\"get\" + 0.002*\"lik\" + 0.002*\"exo\" + 0.002*\"happy\" + 0.002*\"watch\"\n",
      "\n",
      "Score: 0.0333333350718\t \n",
      "Topic: 0.003*\"https\" + 0.003*\"on\" + 0.002*\"check\" + 0.002*\"person\" + 0.002*\"autom\" + 0.002*\"u\" + 0.002*\"new\" + 0.002*\"bless\" + 0.002*\"season\" + 0.002*\"thank\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[814]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test document has the highest probability to be part of the topic on the top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
